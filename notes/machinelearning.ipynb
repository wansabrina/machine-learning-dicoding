{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction untuk mendeteksi objek.\n",
    "\n",
    "Jenis jenis machine learning:\n",
    "1. Supervised learning\n",
    "    dataset yg digunakan memiliki label dan algoritma kemudian mepelajari pola dari pasangan data dan label tersebut.\n",
    "2. Unsupervised learning\n",
    "    tidak ada label di dataset, melakukan proses belajar sendiri untuk melabeli/mengelompokkan data.\n",
    "3. Semi-supervised learning\n",
    "    gabungan dari 1 dan 2, di dataset half half ada label dan tidak\n",
    "4. Reinforcement learning\n",
    "    belajar menggunakan sistem reward dan penalti, belajar melalui proses trial dan error. ada action, agent, environment, dan reward\n",
    "\n",
    "scripting, interactive\n",
    "\n",
    "library ada numpy, pandas, matplotlib, tensorflow, scikit learn, keras dan pytorch\n",
    "\n",
    "Data Collecting\n",
    "sumber untuk data collecting ada UCI ML Repository, kaggle, google dataset search engine, tensorflow dataset, satu data indonesia, US gov data, open data jawa barat\n",
    "\n",
    "Data Cleaning\n",
    "konsistensi format, skala data, duplikasi data, missing value, skewness dist (keseimbangan data, atau untuk menghindari bias dan imbalance data, misal model cenderung memprediksi sesuatukarena ia lebih sering mempelajari hal tsb)\n",
    "\n",
    "Data Processing\n",
    "Cenderung pake pandas, untuk manip tabel numerik dan time series. \n",
    "\n",
    "Data Preparation\n",
    "data ada 2 jenis yaitu numerik dan kategorik\n",
    "ML tidak dapat mengolah data kategorik, jadi harus convert ke numerik, misal regresi linear dan SVM hanya menerima input numerik. Teknik convertnya itu dengan menggunakan \n",
    "- One Hot Encoding atau dummy variables, misal 0 untuk false dan 1 untuk true\n",
    "\n",
    "- Outlier Removal\n",
    "outlier adalah sebuah nilai yang jauh berbeda dari kumpulan nilai lainnya dan dapat mengacaukan hasil dari sebuah analisis statistik.\n",
    "\n",
    "- Normalization\n",
    "tujuannya mengubah nilai nilai dari sebuah fitur ke dalam skala yg sama\n",
    "misal ada 2 fitur contoh gaji dan umur, gaji sampe jutaan angkanya, sedangkan umur puluhan, skalanya jauh beda. kalau bikin model seperti regresi linear, hal ini sangat mempengaruhi hasil prediksi.\n",
    "normalization bisa pake min-max scaling, dimana nilai nilai akan dipetakan ke dalam skala 0 sampai 1, dari lib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25806452 0.45454545]\n",
      " [1.         1.        ]\n",
      " [0.         0.        ]\n",
      " [0.08064516 0.13636364]\n",
      " [0.16129032 0.27272727]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[12000000, 33], [35000000, 45], [4000000, 23], [6500000, 26], [9000000, 29]]\n",
    "\n",
    "scaler = MinMaxScaler() #kita bikin objek minmaxscaler\n",
    "scaler.fit(data) #panggil fungsi fit untuk menghitung nilai minimum dan maksimum pada setiap kolom\n",
    "# komputer baru ngitung nilai min dan max pada tiap kolom dan belum melakukan operasi scaler, panggil\n",
    "# fungsi transform() untuk mengaplikasikan scaler\n",
    "\n",
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardization\n",
    "proses konversi nilai nilai dari suatu fitur sehingga nilai tsb punya skala yg sama. contoh metodenya Z Score dimana setiap nilai pada sebuah atribut numerik akan dikurangi dengan rata2 dan dibagi dengan standar deviasi dari seluruh nilai pada sebuah kolom atribut. serupa dengan normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11638732,  0.23521877],\n",
       "       [ 1.94277296,  1.80334389],\n",
       "       [-0.83261698, -1.07155217],\n",
       "       [-0.60879521, -0.67952089],\n",
       "       [-0.38497344, -0.28748961]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "data = [[12000000, 33], [35000000, 45], [4000000, 23], [6500000, 26], [9000000, 29]]\n",
    "\n",
    "#bisa nulisnya kek yg normalization\n",
    "scaler = preprocessing.StandardScaler().fit(data)\n",
    "data = scaler.transform(data)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Storage/Warehouse\n",
    "- RDBMS (yang serba sql, mysql, postgresql microsoft sql server)\n",
    "- NoSQL (doc, graph, key-value, dan column based) cthnya mongodb, couchdb, cassandra, redis, neo4j dan riak.\n",
    "- Firebase realtime database\n",
    "database yang menyimpan data yang berubah seiring waktu, disimpan dalam format waktu dan nilai pada waktu yg terkait.\n",
    "- Apache Spark\n",
    "perangkat lunak untuk pemrosesan dan analisis data berskala besar. Spark dapat digunakan dalam proses ETL (Extract, Transform, Load), data streaming, perhitungan grafik, SQL, dan machine learning\n",
    "- Biq Query\n",
    "data warehouse berbasis cloud untuk perusahaan yang menawarkan penyimpanan data berbasis SQL dan analisis data berukuran besar. Karena berbasis cloud dan tidak ada infrastruktur yang perlu dikelola, pengguna dapat berfokus pada pengolahan data tanpa memerlukan seorang administrator database. \n",
    "\n",
    "Datasets\n",
    "bagi dataset jadi 2 yaitu training set dan test set, biasanya train set lebih banyak daripada test set(?) pembagian data training dan data testing yang paling umum adalah 80:20, 70:30, atau 60:40 tergantung dari ukuran atau jumlah data. untuk dataset berukuran besar, proporsi pembagian 90:10 atau 99:1.\n",
    "\n",
    "kita bisa pake fungsi train_test_split dari sklearn. sebelum displit, fungsi ini akan mengacak dataset secara internal, jadi merata kelasnya. ada random seed untuk internal pseudo-random generator yang digunakan pada proses shuffling. umumnya nilainya 0 atau 1 atau 42. ada random_state gunanya untuk memastikan bahwa hasil pembagian dataset konsisten dan memberikan data yang sama setiap kali model dijalankan. Jika tidak ditentukan, maka tiap kali melakukan split, kita akan mendapatkan data train dan tes berbeda, yang juga akan membuat akurasi model ML menjadi berbeda tiap kali di-run.  \n",
    "\n",
    "parameter train_Test_split yaitu\n",
    "x = atribut dari dataset\n",
    "y = target dari dataset\n",
    "test_size = persentase test set\n",
    "random_state (1, 0, atau 42 bebas asal konsisten pake yg mana)\n",
    "\n",
    "yang akan mengembalikan 4 nilai yaitu:\n",
    "atribut train set\n",
    "atribut test set\n",
    "target train set\n",
    "target test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X_data = range(10)\n",
    "y_data = range(10)\n",
    " \n",
    "print(\"random_state ditentukan\")\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.4, random_state = 42)\n",
    "    print(y_test)\n",
    " \n",
    " \n",
    "print(\"random_state tidak ditentukan\")\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.4, random_state = None)\n",
    "    print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latihan split\n",
    "import sklearn\n",
    "from sklearn import datasets # lib sklearn menyediakan dataset iris, yg umum digunakan untuk masalah klasifikasi\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "# pisahkan atribut dan label pada dataset\n",
    "\n",
    "x = iris.data # adalah array dgn bentuk n_samples, dan n_Features, dimana sample adalah jumlah sampel/baris, dan features adalah jumlah kolom\n",
    "y = iris.target # atribut berisi label kelas untuk setiap sampel data, misal label 0 kelas A, label 1 kelas B gt gt, arraynya adalah n_samples/baris yg sesuai dengan iris.data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print(len(x))\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Evaluation\n",
    "misal ada 10 jenis model, latih semua model tsb lalu bandingkan tingkat errornya, pililh yg paling kecil untuk tahap produksi. Lalu ketika diuji pada tahap produksi, tingkat errornya membesar, karena kita mengukur tingkat eror berulang kali pada test set, dan scr tidak sadar memilih model yg hanya bekerja dengan baik pada test set tsb, modelnya buruk ketika menemui data baru. solusinya adalah menambahkan validation set.\n",
    "\n",
    "- Train, Test, Validation Set (pembagian datanya tambah 1 lagi)\n",
    "validation/holdout adalah bagian dari train set untuk uji model tahap awal. jangan sampe ukuran validation set nya terlalu besar/kecil. solusinya pake cross validation\n",
    "\n",
    "- Cross Validation\n",
    "K-Fold Cross Validation. dataset akan dibagi sebanyak k lipatan. pada setiap iterasi setiap fold akan dipakai sekali sbg data test dan fold sisanya dibakai sbg data train. \n",
    "\n",
    "pake fungsi cross_val_score(), 4 parameter\n",
    "clf atau classifier\n",
    "x atribut dataset\n",
    "y label dataset\n",
    "cv yaitu jumlah fold yg akan dipake cross validation\n",
    "\n",
    "akan mengembalikan nilai berupa larik/array yg terdiri dari akurasi pengujian setiap fold dari dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.9       , 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn import tree\n",
    "# clf itu classifier, yaitu model machine learning\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, x, y, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elemen pertama dari larik menunjukkan nilai 0.966666667 yg berart ketika fold pertama dijadikan validation set dan fold lainnya dijadikan train set, hasil dari pengujian tsb adalah akurasi sebesar 0.96666. Secara umum jika hasil dari pengujian tiap fold pada cross validation memiliki nilai yang bervariasi dari 0.85 sampai 0.99, maka model tersebut dapat dikatakan baik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
